#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Created on Tue Mar 16 11:18:03 2021@author: criss_94"""import nltkfrom nltk.collocations import *from collections import Counterimport en_core_web_smimport sysfrom sys import exitimport scipy as sp    #functionsdef listToString(s):     str1 = ""     for ele in s:         str1 += ele+" "    return str1 def listToString1(s):     str1 = ""     for ele in s:         str1 += ele+"\n\n"      return str1def findIndexWords(_words,_text):    index=[]    words=_words.split(" ")    text=_text.split(" ")    j=0    for i in range(0,len(text)):        if(text[i]==words[j]):            j=j+1            if(j==2):                index.append(i-2)                j=0        else:            j=0    return indexdef text_entropy(text):    # we only consider UTF8 characters to compute the text entropy    pk = [text.count(chr(i)) for i in range(256)]    if sum(pk) == 0:        text_entropy = None    else:        text_entropy = sp.stats.entropy(pk, base=2)    return text_entropy #classesclass word:    def __init__(self,w,f):        self.w=w        self.f=f            def print(self):        print(self.w+" - " +self.f)    class sentence:    def __init__(self,sent,frequencies):        self._sent=sent        self.sent=sent.split(" ")             self.score=0        self.frequencies=frequencies        self.calculate_score()            def calculate_score(self):        for word in self.sent:            tmpScore=0            for x in self.frequencies:                if x.w==word:                    tmpScore=x.f                    break            self.score+=float(tmpScore)        def _print(self):        print(str(self._sent)+" : "+ str(self.score))        class Summerizer:    def __init__(self,sentences):        self.sentences=sentences        _tokenizedSentence=nltk.word_tokenize(sentences)        self.tokenizedSentence=[]                for x in _tokenizedSentence:            if not any(bad_char in x for bad_char in ["(",")",",",";","[","]","."]) :                self.tokenizedSentence.append(x)                        counter_list=list(Counter(self.tokenizedSentence))        counter=Counter(self.tokenizedSentence)        self.frequency=[]        # self.frequency=[None]*len(counter_list)        if(len(counter.values())>0):        # if(counter.values!=""):            # print(len(counter.values()))            _max=(max(counter.values()))        else:            _max=0        for i in range(0,len(counter_list)):            self.frequency.append(round(int(counter[counter_list[i]])/int(_max),3))                final_words=[]        for i in range(0,len(self.frequency)):            tmp=word(str(counter_list[i]),str(self.frequency[i]))            final_words.append(tmp)        sentences_with_score=[]        #must calculate sentences score for each sentences        for _sentence in self.sentences.split("."):            sentences_with_score.append(sentence(_sentence,final_words))         j=0        first = True        self.summarised=""        while( first or ((float(text_entropy(self.sentences)-0.08) > (float(text_entropy(self.summarised))))and len(self.sentences)>2)):              best_index=0            _max=sentences_with_score[0].score            if(j==4):                break            for i in range(1,len(sentences_with_score)):                                if(sentences_with_score[i].score)>_max:                    _max=sentences_with_score[i].score                    best_index=i                    sentences_with_score[i].score=-1            if(self.summarised!=sentences_with_score[best_index]._sent):                self.summarised=self.summarised+sentences_with_score[best_index]._sent            first =False            j=j+1             def getsummarised(self):        return self.summarised    #maindebug=Falseif("-help" in (sys.argv) or "-h" in (sys.argv) or len(sys.argv)<3):    print("USAGE: python3 Summariser_tool.py -file [file_name] [FLAGS]")    print(" [FLAGS]")    print("    -h -help             :shows this manual")    print("    -v -verbose          :print all comments")    print("    -b -bigram [number]  :the minimum number of occurrences for the bigrams")    print("                          default is 10")    exit()    if ("-verbose" in (sys.argv) or "-v" in (sys.argv) ):    debug=Truebigram_freq=10for x in range(0,len(sys.argv)):    if (sys.argv[x]=="-file"):        nameFile=sys.argv[x+1]    if (sys.argv[x]=="-bigram" or sys.argv[x]=="-b"):        bigram_freq=int(sys.argv[x+1])f = open(nameFile+".txt")txtStandard = f.read()tokens = nltk.word_tokenize(txtStandard.lower())_myStandard=nltk.Text(tokens)bigram_measures = nltk.collocations.BigramAssocMeasures()finder_two = BigramCollocationFinder.from_words(_myStandard)finder_two.apply_freq_filter(bigram_freq)usedBigrams=sorted(finder_two.nbest(bigram_measures.pmi, 1000))stringBigram=[]for single_tuple in usedBigrams:    if not any(bad_char in single_tuple for bad_char in ["(",")","the",",",";","[","]",".","a","it","of","on"]) :        stringBigram.append(single_tuple[0]+" "+single_tuple[1])nlp = en_core_web_sm.load()article = nlp(txtStandard)entityNumber=len(article.ents)stringToSearch=[]#adding bigramsfor x in stringBigram:    if not any(_type in x.split(" ") for _type in ["s","have","has","is","the","and","to","be","as","in","are","paragraph","do","does","article","with","that","or","shall"]):        stringToSearch.append(x) stringsToSearch=list(dict.fromkeys(stringToSearch))# stringsToSearch=sorted(stringsToSearch, key=len)f.close()# print(stringsToSearch)f = open(nameFile+"_summarised.txt", "w")all_index=[]if(debug):    print("Looking for bigrams:\n")    print(stringToSearch)input("Press enter to summarise")stringToSave=""for bigram in stringToSearch:    all_index=findIndexWords(bigram, txtStandard)    if(debug):        print(bigram +": "+str(len(all_index)))    sentencesToSave=[]    for index in all_index:        start=index-100        end=index+100        if (start<0):            start=0                    if(debug):            print("*-*-*-*-*-*-*-*-*-*-*-*-*-*start from this text*-*-*-*-*-*-*-*-*-*-*-*-*-*\n")            print(listToString(_myStandard[start:end]))            print("\n\n\n")                sentencesToAnalyze=(listToString(_myStandard[start:end])).split(".")        if(not(sentencesToAnalyze[0])==""):               sentToSummerize=""            i=0            for i in range (0,len(sentencesToAnalyze)):                if (not(i==0 or i == len(sentencesToAnalyze))):                    text = nltk.word_tokenize(sentencesToAnalyze[i])                    pos_tagged = nltk.pos_tag(text)                    verbs = list(filter(lambda x:x[1]=='VB',pos_tagged))                    verbs = [x[0] for x in verbs]                    if(len(verbs)>0):                        sentToSummerize+=sentencesToAnalyze[i]                        sentToSummerize+="."                i=i+1                        if(debug):                print("*-*-*-*-*-*-*-*-*-*-*-*-*-*after first elaboration*-*-*-*-*-*-*-*-*-*-*-*-*\n")                print(sentToSummerize)                print("\n\n\n--")            if(sentToSummerize!=""):                x=Summerizer(sentToSummerize)                summarised___=x.getsummarised()                if(debug):                    print("*-*-*-*-*-*-*-*-*-*after final elaboration (summarization)-*-*-*-*-*-*-*-*-*\n")                    print(summarised___)                    print("\n\n\n")                                sentencesToSave.append(summarised___)            listToSave=list(dict.fromkeys(sentencesToSave))    # removing redundant paragraph    for paragraph in listToSave:        for paragraph1 in listToSave:            if(paragraph!=paragraph1 and ( paragraph1 in paragraph or paragraph in paragraph1)):                if(len(paragraph)>=len(paragraph1)):                    listToSave.remove((paragraph1))                else:                    listToSave.remove((paragraph1))    stringToSave=stringToSave+listToString1(listToSave)stringToSave=listToString1(list(dict.fromkeys(stringToSave.split("\n"))))f.write(stringToSave)    f.close()f = open(nameFile+"_summarised.txt")sum_ = f.read()print("\nProduced "+nameFile+"_summarised.txt \n")print("Number of character before: "+ str(len(txtStandard)))print("Number of character after: "+ str(len(sum_)))compression=100-((len(sum_)*100/len(txtStandard)))print("With a compression of "+ str(compression)[0:5]+"%")f.close()                    